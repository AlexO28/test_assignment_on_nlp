{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f719ae",
   "metadata": {},
   "source": [
    "# Цель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e5146",
   "metadata": {},
   "source": [
    "Решить задачу классификации с помощью коробочного решения, основанного на трансформерах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da002d6",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2faf26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83206508",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv('file.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c470ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import y_dict\n",
    "CLASSES = [\"\", 'Бизнес-карта', 'Зарплатные проекты', 'Открытие банковского счета', 'Эквайринг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8af0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.reset_index(inplace=True)\n",
    "tab['text'] = tab['text_employer']\n",
    "tab['category'] = tab['ACTION_ITEM_RESULT_PRODUCT_NAME']\n",
    "tab = tab[['text', 'category']]\n",
    "tab['id'] = tab.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb21adb",
   "metadata": {},
   "source": [
    "# Определение структуры трансформеров (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abbb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_cosine_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8564c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, phase='test'):\n",
    "        self.phase = phase\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            self.labels = [labels[label] for label in df['category']]\n",
    "        elif self.phase == 'test':\n",
    "            self.id = [oid for oid in df['id']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.phase == 'train':\n",
    "            return len(self.labels)\n",
    "        elif self.phase == 'test':\n",
    "            return len(self.id)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "    \n",
    "    def get_batch_oid(self, idx):\n",
    "        return np.array(self.id[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.phase == 'train':\n",
    "            batch_texts = self.get_batch_texts(idx)\n",
    "            batch_y = self.get_batch_labels(idx)\n",
    "            return batch_texts, batch_y\n",
    "        elif self.phase == 'test':\n",
    "            batch_texts = self.get_batch_texts(idx)\n",
    "            batch_oid = self.get_batch_oid(idx)\n",
    "            return batch_texts, batch_oid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b74f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier:\n",
    "    def __init__(self, model_path, tokenizer_path, data, n_classes=13, epochs=5):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.data = data\n",
    "        self.device = torch.device('cuda')\n",
    "        self.max_len = 512\n",
    "        self.epochs = epochs\n",
    "        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
    "        self.model.classifier = torch.nn.Linear(self.out_features, n_classes).cuda()\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "    \n",
    "    def preparation(self):\n",
    "        self.df_train, self.df_val, self.df_test = np.split(self.data.sample(frac=1, random_state=42), \n",
    "                                     [int(.85*len(self.data)), int(.95*len(self.data))])\n",
    "        \n",
    "        self.train, self.val = CustomDataset(self.df_train, self.tokenizer, phase='train'), CustomDataset(self.df_val, self.tokenizer, phase='train')\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(self.train, batch_size=4, shuffle=True)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(self.val, batch_size=4)\n",
    "    \n",
    "       \n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=len(self.train_dataloader) * self.epochs\n",
    "            )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "            \n",
    "    def fit(self):\n",
    "        self.model = self.model.train()\n",
    "        \n",
    "        for epoch_num in range(self.epochs):\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            for train_input, train_label in tqdm(self.train_dataloader):\n",
    "                train_label = train_label.cuda()\n",
    "                mask = train_input['attention_mask'].cuda()\n",
    "                input_id = train_input['input_ids'].squeeze(1).cuda()\n",
    "                output = self.model(input_id.cuda(), mask.cuda())\n",
    "\n",
    "                batch_loss = self.loss_fn(output[0], train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                acc = (output[0].argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "            total_acc_val, total_loss_val = self.eval()\n",
    "           \n",
    "            print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(self.df_train): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(self.df_train): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(self.df_val): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(self.df_val): .3f}')\n",
    "\n",
    "            \n",
    "            os.makedirs('checkpoint', exist_ok=True)\n",
    "            torch.save(self.model, f'checkpoint/BertClassifier{epoch_num}.pt')\n",
    "\n",
    "        return total_acc_train, total_loss_train\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model = self.model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in tqdm(self.val_dataloader):\n",
    "                val_label = val_label.cuda()\n",
    "                mask = val_input['attention_mask'].cuda()\n",
    "                input_id = val_input['input_ids'].squeeze(1).cuda()\n",
    "\n",
    "                output = self.model(input_id.to('cuda'), mask.to('cuda'))\n",
    "\n",
    "                batch_loss = self.loss_fn(output[0], val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output[0].argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "            \n",
    "        return total_acc_val, total_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389960fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd77a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(tab, random_state=239, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c970c",
   "metadata": {},
   "source": [
    "# Применение трансформеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed6dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'cointegrated/rubert-tiny2'\n",
    "tokenizer_path = 'cointegrated/rubert-tiny2'\n",
    "bert_tiny = BertClassifier(model_path, tokenizer_path, train_val, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754315c",
   "metadata": {},
   "source": [
    "Мы выбираем не большой BERT, а его дистиллированную и обработанную версию, чтобы получить результаты побыстрее, пожертвовав, возможно точностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62963ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f782d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\playground\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_tiny.preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e50d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [01:17<00:00, 11.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 47.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.200             | Train Accuracy:  0.693             | Val Loss:  0.154             | Val Accuracy:  0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [01:07<00:00, 13.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 47.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.138             | Train Accuracy:  0.775             | Val Loss:  0.145             | Val Accuracy:  0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [01:07<00:00, 13.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.115             | Train Accuracy:  0.825             | Val Loss:  0.148             | Val Accuracy:  0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [01:11<00:00, 13.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:01<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.090             | Train Accuracy:  0.867             | Val Loss:  0.157             | Val Accuracy:  0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [00:58<00:00, 15.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:01<00:00, 55.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.067             | Train Accuracy:  0.905             | Val Loss:  0.167             | Val Accuracy:  0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [00:58<00:00, 15.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:01<00:00, 56.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.051             | Train Accuracy:  0.932             | Val Loss:  0.175             | Val Accuracy:  0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [00:58<00:00, 15.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:01<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.039             | Train Accuracy:  0.950             | Val Loss:  0.185             | Val Accuracy:  0.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [00:59<00:00, 15.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 54.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.032             | Train Accuracy:  0.960             | Val Loss:  0.200             | Val Accuracy:  0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [00:59<00:00, 15.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 53.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.028             | Train Accuracy:  0.965             | Val Loss:  0.197             | Val Accuracy:  0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 927/927 [01:00<00:00, 15.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 109/109 [00:02<00:00, 54.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.026             | Train Accuracy:  0.968             | Val Loss:  0.198             | Val Accuracy:  0.745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3587, 96.23585230961908)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tiny.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a8035",
   "metadata": {},
   "source": [
    "Таким образом, на валидационном множестве получается порядка 76.1%\n",
    "\n",
    "Так как у catboost, и у трансформеров разные валидационные множества сделаем сравнение на тестовом множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aaa4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test, bert_tiny.tokenizer, phase='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdfe1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dataloader):\n",
    "    all_oid = []\n",
    "    all_labels = []\n",
    "    label_prob = []\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_oid in tqdm(dataloader):\n",
    "            test_oid = test_oid.cuda()\n",
    "            mask = test_input['attention_mask'].cuda()\n",
    "            input_id = test_input['input_ids'].squeeze(1).cuda()\n",
    "            output = model(input_id, mask)\n",
    "            all_oid.extend(test_oid)\n",
    "            all_labels.extend(torch.argmax(output[0].softmax(1), dim=1))\n",
    "            \n",
    "            for prob in output[0].softmax(1):\n",
    "                label_prob.append(prob)\n",
    "        return ([oid.item() for oid in all_oid], [CLASSES[labels] for labels in all_labels], label_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cfb6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = torch.load('./checkpoint/BertClassifier2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc2cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 273/273 [00:04<00:00, 56.52it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_result = inference(inference_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c20f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [i for i in inference_result[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07705201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7568807339449541"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[test['category'] == new_labels])/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9117eb",
   "metadata": {},
   "source": [
    "Итак, 75.69%\n",
    "\n",
    "Результат получился сравнимый с catboost, но всё-таки похуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5274d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
